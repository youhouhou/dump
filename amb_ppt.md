## Problem context

 a fundamental challenge arises from the inherent nature of human language: ambiguity. User queries in conversational settings are frequently underspecified, vague, or contain elements that allow for multiple interpretations. While ambiguity facilitates concise communication between humans who share context , it presents significant obstacles for AI systems striving for accurate information retrieval, task completion, or response generation.

Human language is inherently fluid and often ambiguous, a characteristic that poses significant challenges for artificial intelligence (AI) systems designed to engage in natural conversation. Ambiguity, defined as an expression carrying multiple distinct meanings or interpretations , frequently arises from communicative efficiency pressures, such as the use of ellipsis (omitting words) or imprecise phrasing. While humans adeptly navigate ambiguity using shared context, world knowledge, and interactive clarification , Large Language Models (LLMs) often struggle.

This struggle manifests in various ways that degrade the user experience and system reliability. Misinterpreting ambiguous user queries can lead LLMs to provide irrelevant or incorrect responses, generate plausible but factually wrong information (hallucinations), or exhibit biased outputs, ultimately eroding user trust and satisfaction. The challenge intensifies in multi-turn conversations, where maintaining context, tracking evolving user intentions, and resolving dependencies across turns become increasingly complex. Effectively managing ambiguity is therefore a critical frontier in advancing conversational AI. 

### where does ambiguity come from
Formally, ambiguity occurs when a linguistic expression possesses two or more distinct denotations or interpretations. In conversational AI, ambiguity can originate from various sources. User utterances may be ambiguous due to intentional brevity, ellipsis, or imprecision aimed at efficient communication , or unintentionally due to the user's lack of specific domain knowledge. Ambiguity can also stem from the task instructions provided to the AI if they are unclear or underspecified.

## Prob sizing
### LLM judge
Given the potential for biases and errors, the classifications produced by the LLM judge cannot be taken at face value. Rigorous validation against human expert judgment is essential.   

Process: A statistically significant sample of the LLM judge's classifications (covering both cases flagged as ambiguous and those not) must be reviewed by human experts knowledgeable about the chatbot's domain and conversational AI principles.
Metrics: Standard classification metrics (Accuracy, Precision, Recall, F1-score) should be calculated to quantify the judge's performance on this specific ambiguity detection task. Inter-annotator agreement metrics (e.g., Cohen's Kappa) should be used if multiple human annotators are involved to establish the reliability of the human baseline.   
Iteration: The validation results should inform refinements to the LLM judge's prompt (e.g., clarifying the ambiguity definition, adding better few-shot examples) or even the choice of judge model itself.
Existing Frameworks: While general benchmarks for LLM judges exist , they may not perfectly reflect performance on this specific, nuanced task. Custom validation is crucial.

### need calibration - thats why need annotation
The effectiveness of the LLM judge hinges critically on task-specific calibration. The definition of ambiguity used in the prompt must align precisely with the types of ambiguity the disambiguation module is designed to address. A generic LLM might excel at identifying general linguistic ambiguity but fail to recognize when a user query is ambiguous relative to the specific parameters required by the chatbot's underlying functions or knowledge base. For instance, a query like "check my balance" is only ambiguous if the user has multiple accounts and the chatbot requires account specification. The prompt and few-shot examples must guide the judge to focus on these task-relevant ambiguities. 

## Eval dataset

the type of ambiguous query in early turn vs query in later turn are actually different.

I think we should focus on early turn.

Early Turns (e.g., Turns 1-2): Likely characterized by underspecified queries where the user hasn't provided enough detail. Task ambiguity (unclear goals or constraints) may be high. Lexical and syntactic ambiguities might be present but are less constrained by context. Clarification needs often revolve around defining the core intent, scope, or key entities.
Mid-Turns (e.g., Turns 3-5): Context is actively being established. Ambiguity might arise when refining parameters, comparing alternatives generated by the system, or shifting to closely related sub-topics. Coreference chains begin to form and lengthen, increasing the potential for referential ambiguity. Clarification might focus on specific details or choices.
Later Turns (e.g., Turns 6+): The conversation heavily relies on the accumulated context. Pragmatic ambiguities, especially coreference ("it", "that", "the previous one") and ellipsis referencing earlier information, become prominent challenges. Ambiguity can also arise from complex reasoning spanning multiple previous turns or subtle topic shifts. Clarification might be needed for fine-grained distinctions, confirmation of understanding before taking a final action, or resolving references that span several turns back.


## next step

### why may need sft / RL

A crucial concept emerging from recent research is "Perceived Ambiguity". This refers to the extent to which a specific AI model detects ambiguity in an input, which is contingent upon the model's internal knowledge base. A query like "Which team won the national championship?" might be clearly ambiguous to a model possessing broad knowledge of different sports and their respective championships. However, the same query could be perceived as unambiguous by a model with limited knowledge, perhaps only aware of one prominent "national championship". This distinction is significant because it implies that evaluating ambiguity based solely on an objective ground truth might not fully capture whether the target LLM can recognize and appropriately handle ambiguity given its own knowledge limitations. The model's internal state and its ability to recognize multiple potential meanings based on that state are paramount. This suggests that evaluation methodologies should ideally incorporate ways to assess the model's perspective, possibly through measuring its output uncertainty  or its performance on self-disambiguation tasks.